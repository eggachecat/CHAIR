{
    // Use IntelliSense to learn about possible attributes.
    // Hover to view descriptions of existing attributes.
    // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387
    "version": "0.2.0",
    "configurations": [
        {
            "name": "dola-tqa-mc-dola-with-pyvene",
            "type": "debugpy",
            "request": "launch",
            "cwd": "${workspaceFolder}/DoLA",
            "program": "tfqa_mc_eval_with_pyvene.py",
            "console": "integratedTerminal",
            "python": "${config:the_anaconda_folder}/envs/eggachecat_llm_eval/bin/python",
            "env": {
                "CUDA_VISIBLE_DEVICES": "0",
                "PYTHONPATH": "${workspaceFolder}/pyvene:${workspaceFolder}/LLaMA-Factory:${workspaceFolder}/LLaMA-Factory/src:${env:PYTHONPATH}"
            },
            "args": [
                "--model-name",
                "${workspaceFolder}/downloaded_models/Meta-Llama-3-8B-Instruct",
                "--data-path",
                "${workspaceFolder}/TruthfulQA",
                "--num-gpus",
                "1",
                "--early-exit-layers",
                "16,18,20,22,24,26,28,30,32",
                "--output-path",
                "${workspaceFolder}/saves/evaluation/dola/output-path-tfmc-dola.json"
            ]
        },
        {
            "name": "dola-tqa-mc-dola",
            "type": "debugpy",
            "request": "launch",
            "cwd": "${workspaceFolder}/DoLA",
            "program": "tfqa_mc_eval.py",
            "console": "integratedTerminal",
            "python": "${config:the_anaconda_folder}/envs/eggachecat_llm_dola/bin/python",
            "env": {
                "CUDA_VISIBLE_DEVICES": "0",
                "PYTHONPATH": "${workspaceFolder}/pyvene:${workspaceFolder}/LLaMA-Factory:${workspaceFolder}/LLaMA-Factory/src:${env:PYTHONPATH}"
            },
            "args": [
                "--model-name",
                "huggyllama/Meta-Llama-3-8B-Instruct",
                "--data-path",
                "${workspaceFolder}/TruthfulQA",
                "--num-gpus",
                "1",
                "--early-exit-layers",
                "16,18,20,22,24,26,28,30,32",
                "--output-path",
                "${workspaceFolder}/saves/evaluation/dola/output-path-tfmc-dola.json"
            ]
        },
        {
            "name": "project_contrast_distance_as_prompt_ranker: step-2",
            "type": "debugpy",
            "request": "launch",
            "cwd": "${workspaceFolder}/project_contrast_distance_as_prompt_ranker",
            "program": "step_2_generate_predicted_prompt_rank_by_contrast_distance.py",
            "console": "integratedTerminal",
            "env": {
                "CUDA_VISIBLE_DEVICES": "0",
                "PYTHONPATH": "${workspaceFolder}/pyvene:${workspaceFolder}/LLaMA-Factory:${workspaceFolder}/LLaMA-Factory/src:${env:PYTHONPATH}"
            },
        },
        {
            "name": "Eggachecat: evaluate-model-judge",
            "type": "debugpy",
            "request": "launch",
            "module": "truthfulqa.evaluate_eggachecat_v2_judge",
            "console": "integratedTerminal",
            "cwd": "${workspaceFolder}/TruthfulQA",
            "python": "${config:the_anaconda_folder}/envs/eggachecat_llm/bin/python",
            "justMyCode": false,
            "env": {
                "CUDA_VISIBLE_DEVICES": "0",
                "PYTHONPATH": "${workspaceFolder}/LLaMA-Factory:${workspaceFolder}/LLaMA-Factory/src:${env:PYTHONPATH}"
            },
            "args": [
                "--input_path",
                "TruthfulQA_answers.csv",
                "--output_path",
                "TruthfulQA_answers_with_metrics.csv",
                "--tag",
                "Mistral-7B-Instruct-v0.3-judge-info",
                "--model_name_or_path",
                "${workspaceFolder}/downloaded_models/Mistral-7B-Instruct-v0.3",
                "--adapter_name_or_path",
                "${workspaceFolder}/saves/models/Mistral-7B-Instruct-v0.3/truthful_qa_judger_info/lora/sft",
                "--template",
                "mistral",
                "--metrics",
                "mc",
                "--metrics",
                "bleu",
                "--metrics",
                "bleurt",
                "--metrics",
                "info",
                "--task",
                "eval"
            ]
        },
        {
            "name": "LLaMAFactory: API",
            "type": "debugpy",
            "request": "launch",
            "program": "${workspaceFolder}/LLaMA-Factory/src/llamafactory/eggachecat_main.py",
            "console": "integratedTerminal",
            "cwd": "${workspaceFolder}",
            "python": "${config:the_anaconda_folder}/envs/eggachecat_llm/bin/python",
            "justMyCode": false,
            "env": {
                "API_PORT": "8000",
                "PYTHONPATH": "${workspaceFolder}/LLaMA-Factory:${workspaceFolder}/LLaMA-Factory/src:${env:PYTHONPATH}"
            },
            "args": [
                "api",
                "api_serve_llama3.yaml"
            ]
        },
        {
            "name": "LLaMAFactory: SFT",
            "type": "debugpy",
            "request": "launch",
            "program": "${workspaceFolder}/LLaMA-Factory/src/llamafactory/eggachecat_main.py",
            "console": "integratedTerminal",
            "cwd": "${workspaceFolder}/LLaMA-Factory",
            "python": "${config:the_anaconda_folder}/envs/eggachecat_llm_train/bin/python",
            "justMyCode": false,
            "args": [
                "train",
                "--stage",
                "sft",
                "--do_train",
                "--model_name_or_path",
                "${workspaceFolder}/downloaded_models/Meta-Llama-3-8B-Instruct",
                "--dataset",
                "identity",
                "--dataset_dir",
                "./data",
                "--template",
                "llama3",
                "--finetuning_type",
                "lora",
                "--output_dir",
                "./saves/LLaMA3-8B/lora/sft",
                "--overwrite_cache",
                "--overwrite_output_dir",
                "--cutoff_len",
                "1024",
                "--preprocessing_num_workers",
                "16",
                "--per_device_train_batch_size",
                "2",
                "--per_device_eval_batch_size",
                "1",
                "--gradient_accumulation_steps",
                "8",
                "--lr_scheduler_type",
                "cosine",
                "--logging_steps",
                "10",
                "--warmup_steps",
                "20",
                "--save_steps",
                "10",
                "--eval_steps",
                "10",
                "--evaluation_strategy",
                "steps",
                "--load_best_model_at_end",
                "--learning_rate",
                "5e-5",
                "--num_train_epochs",
                "50.0",
                "--max_samples",
                "1000",
                "--val_size",
                "0.1",
                "--plot_loss",
                "--fp16"
            ]
        },
        {
            "name": "LLaMAFactory: SFT with Pay Attention",
            "type": "debugpy",
            "request": "launch",
            "program": "${workspaceFolder}/LLaMA-Factory/src/llamafactory/eggachecat_main.py",
            "console": "integratedTerminal",
            "cwd": "${workspaceFolder}/LLaMA-Factory",
            "python": "${config:the_anaconda_folder}/envs/eggachecat_llm_train/bin/python",
            "justMyCode": false,
            "args": [
                "train",
                "--stage",
                "sft",
                "--do_train",
                "--model_name_or_path",
                "${workspaceFolder}/downloaded_models/Meta-Llama-3-8B-Instruct",
                "--dataset",
                "identity",
                "--dataset_dir",
                "./data",
                "--template",
                "llama3",
                "--finetuning_type",
                "lora",
                "--lora_target_all_except",
                "focuse_attention_layer",
                "--additional_target",
                "focuse_attention_layer",
                "--output_dir",
                "./saves/LLaMA3-8B/lora/sft",
                "--overwrite_cache",
                "--overwrite_output_dir",
                "--cutoff_len",
                "1024",
                "--preprocessing_num_workers",
                "16",
                "--per_device_train_batch_size",
                "2",
                "--per_device_eval_batch_size",
                "1",
                "--gradient_accumulation_steps",
                "8",
                "--lr_scheduler_type",
                "cosine",
                "--logging_steps",
                "500",
                "--warmup_steps",
                "20",
                "--save_steps",
                "500",
                "--eval_steps",
                "500",
                "--evaluation_strategy",
                "steps",
                "--load_best_model_at_end",
                "--learning_rate",
                "5e-2",
                "--num_train_epochs",
                "50000.0",
                "--max_samples",
                "1000",
                "--val_size",
                "0.1",
                "--plot_loss",
                "--fp16"
            ]
        },
        {
            "name": "LLaMAFactory: Meta-Llama-3-8B-Instruct/wikiqa/pay_attention/sft",
            "type": "debugpy",
            "request": "launch",
            "program": "${workspaceFolder}/LLaMA-Factory/src/llamafactory/eggachecat_main.py",
            "console": "integratedTerminal",
            "python": "${config:the_anaconda_folder}/envs/eggachecat_llm_train/bin/python",
            "justMyCode": false,
            "args": [
                "train",
                "--stage",
                "sft",
                "--do_train",
                "--model_name_or_path",
                "${workspaceFolder}/downloaded_models/Meta-Llama-3-8B-Instruct",
                "--dataset",
                "wikiqa",
                "--dataset_dir",
                "${workspaceFolder}/LLaMA-Factory/data",
                "--template",
                "llama3",
                "--finetuning_type",
                "lora",
                "--lora_target_all_except",
                "focuse_attention_layer",
                "--additional_target",
                "focuse_attention_layer",
                "--output_dir",
                "${workspaceFolder}/saves/Meta-Llama-3-8B-Instruct/wikiqa/pay_attention/sft",
                "--overwrite_cache",
                "--overwrite_output_dir",
                "--cutoff_len",
                "1024",
                "--preprocessing_num_workers",
                "16",
                "--per_device_train_batch_size",
                "2",
                "--per_device_eval_batch_size",
                "1",
                "--gradient_accumulation_steps",
                "8",
                "--lr_scheduler_type",
                "cosine",
                "--logging_steps",
                "500",
                "--warmup_steps",
                "20",
                "--save_steps",
                "500",
                "--eval_steps",
                "500",
                "--evaluation_strategy",
                "steps",
                "--load_best_model_at_end",
                "--learning_rate",
                "5e-2",
                "--num_train_epochs",
                "50000.0",
                "--max_samples",
                "100000",
                "--val_size",
                "0.1",
                "--plot_loss",
                "--fp16"
            ]
        },
        {
            "name": "LLaMAFactory: Meta-Llama-3-8B-Instruct/wikiqa_full/pay_attention/sft",
            "type": "debugpy",
            "request": "launch",
            "program": "${workspaceFolder}/LLaMA-Factory/src/llamafactory/eggachecat_main.py",
            "console": "integratedTerminal",
            "python": "${config:the_anaconda_folder}/envs/eggachecat_llm_train/bin/python",
            "justMyCode": false,
            "args": [
                "train",
                "--stage",
                "sft",
                "--do_train",
                "--model_name_or_path",
                "${workspaceFolder}/downloaded_models/Meta-Llama-3-8B-Instruct",
                "--dataset",
                "wikiqa",
                "--dataset_dir",
                "${workspaceFolder}/LLaMA-Factory/data",
                "--template",
                "llama3",
                "--finetuning_type",
                "lora",
                "--lora_target_all_except",
                "focuse_attention_layer",
                "--additional_target",
                "focuse_attention_layer",
                "--output_dir",
                "${workspaceFolder}/saves/Meta-Llama-3-8B-Instruct/wikiqa_full/pay_attention/sft",
                "--overwrite_cache",
                "--overwrite_output_dir",
                "--cutoff_len",
                "1024",
                "--preprocessing_num_workers",
                "16",
                "--per_device_train_batch_size",
                "2",
                "--per_device_eval_batch_size",
                "1",
                "--gradient_accumulation_steps",
                "8",
                "--lr_scheduler_type",
                "cosine",
                "--logging_steps",
                "500",
                "--warmup_steps",
                "20",
                "--save_steps",
                "500",
                "--eval_steps",
                "500",
                "--evaluation_strategy",
                "steps",
                "--load_best_model_at_end",
                "--learning_rate",
                "5e-2",
                "--num_train_epochs",
                "50.0",
                "--max_samples",
                "100000",
                "--val_size",
                "0.1",
                "--plot_loss",
                "--fp16"
            ]
        },
        {
            "name": "LLaMAFactory: SFT with Pay Attention::debug",
            "type": "debugpy",
            "request": "launch",
            "program": "${workspaceFolder}/LLaMA-Factory/src/llamafactory/eggachecat_main.py",
            "console": "integratedTerminal",
            "cwd": "${workspaceFolder}/LLaMA-Factory",
            "python": "${config:the_anaconda_folder}/envs/eggachecat_llm_train/bin/python",
            "justMyCode": false,
            "args": [
                "train",
                "--stage",
                "sft",
                "--do_train",
                "--model_name_or_path",
                "${workspaceFolder}/downloaded_models/Meta-Llama-3-8B-Instruct",
                "--dataset",
                "identity",
                "--dataset_dir",
                "./data",
                "--template",
                "llama3",
                "--finetuning_type",
                "lora",
                "--lora_target_all_except",
                "focuse_attention_layer",
                "--additional_target",
                "focuse_attention_layer",
                "--output_dir",
                "./saves/LLaMA3-8B/lora_pay_attention/sft",
                "--overwrite_cache",
                "--overwrite_output_dir",
                "--cutoff_len",
                "1024",
                "--preprocessing_num_workers",
                "16",
                "--per_device_train_batch_size",
                "2",
                "--per_device_eval_batch_size",
                "1",
                "--gradient_accumulation_steps",
                "8",
                "--lr_scheduler_type",
                "cosine",
                "--logging_steps",
                "50",
                "--warmup_steps",
                "20",
                "--save_steps",
                "50",
                "--eval_steps",
                "50",
                "--evaluation_strategy",
                "steps",
                "--load_best_model_at_end",
                "--learning_rate",
                "5e-1",
                "--num_train_epochs",
                "50.0",
                "--max_samples",
                "1000",
                "--val_size",
                "0.1",
                "--plot_loss",
                "--fp16"
            ]
        },
        {
            "name": "LLaMAFactory: Webchat with Pay Attention",
            "type": "debugpy",
            "request": "launch",
            "program": "${workspaceFolder}/LLaMA-Factory/src/llamafactory/eggachecat_main.py",
            "console": "integratedTerminal",
            "cwd": "${workspaceFolder}/LLaMA-Factory",
            "python": "${config:the_anaconda_folder}/envs/eggachecat_llm_train/bin/python",
            "justMyCode": false,
            "args": [
                "webchat",
                "--model_name_or_path",
                "${workspaceFolder}/downloaded_models/Meta-Llama-3-8B-Instruct",
                "--template",
                "llama3",
                "--adapter_name_or_path",
                "./saves/LLaMA3-8B/lora_pay_attention/sft",
            ]
        },
        {
            "name": "LLaMAFactory: Benchmark with Pay Attention",
            "type": "debugpy",
            "request": "launch",
            "program": "${workspaceFolder}/LLaMA-Factory/src/llamafactory/eggachecat_main.py",
            "console": "integratedTerminal",
            "cwd": "${workspaceFolder}/LLaMA-Factory",
            "python": "${config:the_anaconda_folder}/envs/eggachecat_llm_train/bin/python",
            "env": {
                "USE_TORCH": "TRUE"
            },
            "justMyCode": false,
            "args": [
                "eval",
                "--model_name_or_path",
                "${workspaceFolder}/downloaded_models/Meta-Llama-3-8B-Instruct",
                "--template",
                "llama3",
                "--adapter_name_or_path",
                "./saves/LLaMA3-8B/lora_pay_attention/sft",
                "--task_dir",
                "${workspaceFolder}/LLaMA-Factory/evaluation",
                "--task",
                "mmlu_test",
                "--lang",
                "en",
                "--n_shot",
                "5",
                "--batch_size",
                "1"
            ]
        },
        {
            "name": "LLaMAFactory: Benchmark Meta-Llama-3-8B-Instruct/wikiqa_full/pay_attention/sft",
            "type": "debugpy",
            "request": "launch",
            "program": "${workspaceFolder}/LLaMA-Factory/src/llamafactory/eggachecat_main.py",
            "console": "integratedTerminal",
            "cwd": "${workspaceFolder}/LLaMA-Factory",
            "python": "${config:the_anaconda_folder}/envs/eggachecat_llm_train/bin/python",
            "env": {
                "USE_TORCH": "TRUE"
            },
            "justMyCode": false,
            "args": [
                "eval",
                "--model_name_or_path",
                "${workspaceFolder}/downloaded_models/Meta-Llama-3-8B-Instruct",
                "--template",
                "llama3",
                "--adapter_name_or_path",
                "${workspaceFolder}/saves/Meta-Llama-3-8B-Instruct/wikiqa_full/pay_attention/sft",
                "--task_dir",
                "${workspaceFolder}/LLaMA-Factory/evaluation",
                "--task",
                "mmlu_test",
                "--lang",
                "en",
                "--n_shot",
                "5",
                "--batch_size",
                "1"
            ]
        },
        {
            "name": "LLaMAFactory and TruthfulQA: Get Response From Meta-Llama-3-8B-Instruct/wikiqa_full/pay_attention/sft",
            "type": "debugpy",
            "request": "launch",
            "module": "truthfulqa.evaluate_eggachecat_v2_get_response",
            "console": "integratedTerminal",
            "cwd": "${workspaceFolder}/TruthfulQA",
            "python": "${config:the_anaconda_folder}/envs/eggachecat_llm_eval/bin/python",
            "env": {
                "USE_TORCH": "TRUE"
            },
            "justMyCode": false,
            "args": [
                "--input_path",
                "TruthfulQA.csv",
                "--output_path",
                "${workspaceFolder}/saves/evaluation/wikiqa_full_pay_attention_debug/TruthfulQA_answers.csv",
                "--tag",
                "wikiqa_full_pay_attention_debug",
                "--template",
                "llama3",
                "--model_name_or_path",
                "${workspaceFolder}/downloaded_models/Meta-Llama-3-8B-Instruct",
                "--adapter_name_or_path",
                "${workspaceFolder}/saves/Meta-Llama-3-8B-Instruct/wikiqa_full/pay_attention/sft",
                "--task",
                "eval"
            ]
        },
        {
            "name": "LLaMAFactory and TruthfulQA: Judge Base From Meta-Llama-3-8B-Instruct/wikiqa_full/pay_attention/sft",
            "type": "debugpy",
            "request": "launch",
            "module": "truthfulqa.evaluate_eggachecat_v2_judge",
            "console": "integratedTerminal",
            "cwd": "${workspaceFolder}/TruthfulQA",
            "python": "${config:the_anaconda_folder}/envs/eggachecat_llm_eval/bin/python",
            "env": {
                "USE_TORCH": "TRUE"
            },
            "justMyCode": false,
            "args": [
                "--input_path",
                "${workspaceFolder}/saves/evaluation/wikiqa_full_pay_attention_debug/TruthfulQA_answers.csv",
                "--output_path",
                "${workspaceFolder}/saves/evaluation/wikiqa_full_pay_attention_debug/TruthfulQA_answers_with_metrics_base.csv",
                "--model_key",
                "wikiqa_full_pay_attention_debug",
                "--metrics",
                "mc",
                "--metrics",
                "bleu",
                "--metrics",
                "bleurt",
            ]
        },
        {
            "name": "LLaMAFactory and DoLA: Dola With Meta-Llama-3-8B-Instruct/wikiqa_full/pay_attention/sft",
            "type": "debugpy",
            "request": "launch",
            "program": "${workspaceFolder}/DoLA/tfqa_mc_eval_with_llama_factory_and_pyvene.py",
            "console": "integratedTerminal",
            "cwd": "${workspaceFolder}/DoLA",
            "python": "${config:the_anaconda_folder}/envs/eggachecat_llm_eval/bin/python",
            "env": {
                "PYTHONPATH": "${workspaceFolder}/pyvene:${workspaceFolder}/LLaMA-Factory:${workspaceFolder}/LLaMA-Factory/src:${env:PYTHONPATH}"
            },
            "justMyCode": false,
            "args": [
                "--model_name_or_path",
                "${workspaceFolder}/downloaded_models/Meta-Llama-3-8B-Instruct",
                "--adapter_name_or_path",
                "${workspaceFolder}/saves/Meta-Llama-3-8B-Instruct/wikiqa_full/pay_attention/sft",
                "--data-path",
                "${workspaceFolder}/TruthfulQA",
                "--output-path",
                "${workspaceFolder}/saves/evaluation/dola-attention-of-attention/dola-tqa-mc-dola-pyvene-with-wiki-qa.json",
                "--early-exit-layers", "16,18,20,22,24,26,28,30,32",
                "--task", "eval"
            ]
        },
        {
            "name": "LLaMAFactory and DoLA: Dola Observe Meta-Llama-3-8B-Instruct",
            "type": "debugpy",
            "request": "launch",
            "program": "${workspaceFolder}/DoLA/tfqa_mc_eval_with_early_decoding.py",
            "console": "integratedTerminal",
            "cwd": "${workspaceFolder}/DoLA",
            "python": "${config:the_anaconda_folder}/envs/eggachecat_llm_eval/bin/python",
            "env": {
                "PYTHONPATH": "${workspaceFolder}/pyvene:${workspaceFolder}/LLaMA-Factory:${workspaceFolder}/LLaMA-Factory/src:${env:PYTHONPATH}"
            },
            "justMyCode": false,
            "args": [
                "--model_name_or_path",
                "${workspaceFolder}/downloaded_models/Meta-Llama-3-8B-Instruct",
                "--data-path",
                "${workspaceFolder}/TruthfulQA",
                "--output-path",
                "${workspaceFolder}/saves/evaluation/dola-oberservation-layer-debug/layer-1.json",
                "--early-exit-layers", "1",
                "--task", "eval"
            ]
        },
        {
            "name": "DoLA: factor_eval Meta-Llama-3-8B-Instruct",
            "type": "debugpy",
            "request": "launch",
            "program": "${workspaceFolder}/DoLA/factor_eval_with_pyvene.py",
            "console": "integratedTerminal",
            "cwd": "${workspaceFolder}/DoLA",
            "python": "${config:the_anaconda_folder}/envs/eggachecat_llm_eval/bin/python",
            "env": {
                "PYTHONPATH": "${workspaceFolder}/pyvene:${workspaceFolder}/LLaMA-Factory:${workspaceFolder}/LLaMA-Factory/src:${env:PYTHONPATH}"
            },
            "justMyCode": false,
            "args": [
                "--model-name",
                "${workspaceFolder}/downloaded_models/Meta-Llama-3-8B-Instruct",
                "--data-path",
                "${workspaceFolder}/saves/wiki_factor/wiki_factor.csv",
                "--output-path",
                "${workspaceFolder}/saves/evaluation/factor_eval/wiki_factor/output-path-factor-wiki-dola-early-exit.json",
                // "--early-exit-layers", "2,4,6,8,10,12,14,32",
                // "--task", "eval"
            ]
        },
        {
            "name": "DoLA: Eggachecat",
            "type": "debugpy",
            "request": "launch",
            "program": "${workspaceFolder}/DoLA/tfqa_mc_eval_with_eggachecat.py",
            "console": "integratedTerminal",
            "cwd": "${workspaceFolder}/DoLA",
            "python": "${config:the_anaconda_folder}/envs/eggachecat_llm_eval/bin/python",
            "env": {
                "PYTHONPATH": "${workspaceFolder}:${workspaceFolder}/pyvene:${workspaceFolder}/LLaMA-Factory:${workspaceFolder}/LLaMA-Factory/src:${env:PYTHONPATH}"
            },
            "justMyCode": false,
            "args": [
                "--model_name_or_path",
                "${workspaceFolder}/downloaded_models/Meta-Llama-3-8B-Instruct",
                "--data-path",
                "${workspaceFolder}/TruthfulQA",
                "--output-path",
                "${workspaceFolder}/saves/evaluation/dola-eggachecat/result.json",
                "--task", "eval"
            ]
        },
        {
            "name": "DoLA: factor_eval with Eggachecat",
            "type": "debugpy",
            "request": "launch",
            "program": "${workspaceFolder}/DoLA/factor_eval_with_eggachecat.py",
            "console": "integratedTerminal",
            "cwd": "${workspaceFolder}/DoLA",
            "python": "${config:the_anaconda_folder}/envs/eggachecat_llm_eval/bin/python",
            "env": {
                "PYTHONPATH": "${workspaceFolder}:${workspaceFolder}/pyvene:${workspaceFolder}/LLaMA-Factory:${workspaceFolder}/LLaMA-Factory/src:${env:PYTHONPATH}"
            },
            "justMyCode": false,
            "args": [
                "--model_name_or_path",
                "${workspaceFolder}/downloaded_models/Meta-Llama-3-8B-Instruct",
                "--data-path",
                "${workspaceFolder}/saves/wiki_factor/wiki_factor.csv",
                "--output-path",
                "${workspaceFolder}/saves/evaluation/factor_eval/wiki_factor/output-path-factor-wiki-dola-eggachecat.json",
                "--task", "eval",
                "--template", "llama3"
            ]
        },
        {
            "name": "DoLA: factor_eval with EggachecatV2",
            "type": "debugpy",
            "request": "launch",
            "program": "${workspaceFolder}/DoLA/factor_eval_with_eggachecat_v2.py",
            "console": "integratedTerminal",
            "cwd": "${workspaceFolder}/DoLA",
            "python": "${config:the_anaconda_folder}/envs/eggachecat_llm_eval/bin/python",
            "env": {
                "PYTHONPATH": "${workspaceFolder}:${workspaceFolder}/pyvene:${workspaceFolder}/LLaMA-Factory:${workspaceFolder}/LLaMA-Factory/src:${env:PYTHONPATH}"
            },
            "justMyCode": false,
            "args": [
                "--model_name_or_path",
                "${workspaceFolder}/downloaded_models/Meta-Llama-3-8B-Instruct",
                "--data-path",
                "${workspaceFolder}/saves/wiki_factor/wiki_factor.csv",
                "--output-path",
                "${workspaceFolder}/saves/evaluation/factor_eval/wiki_factor/output-path-factor-wiki-dola-eggachecat.json",
                "--task", "eval",
                "--template", "llama3"
            ]
        },
        {
            "name": "DoLA: factor_eval (cross section sum) with EggachecatV2",
            "type": "debugpy",
            "request": "launch",
            "program": "${workspaceFolder}/DoLA/factor_eval_with_eggachecat_per_token_cross_section_sum.py",
            "console": "integratedTerminal",
            "cwd": "${workspaceFolder}/DoLA",
            "python": "${config:the_anaconda_folder}/envs/eggachecat_llm_eval/bin/python",
            "env": {
                "PYTHONPATH": "${workspaceFolder}:${workspaceFolder}/pyvene:${workspaceFolder}/LLaMA-Factory:${workspaceFolder}/LLaMA-Factory/src:${env:PYTHONPATH}"
            },
            "justMyCode": false,
            "args": [
                "--model_name_or_path",
                "${workspaceFolder}/downloaded_models/Meta-Llama-3-8B-Instruct",
                "--data-path",
                "${workspaceFolder}/saves/wiki_factor/wiki_factor.csv",
                "--output-path",
                "${workspaceFolder}/saves/evaluation/factor_eval/wiki_factor/output-path-factor-wiki-dola-eggachecat-cross-section-sum.json",
                "--task", "eval",
                "--template", "llama3"
            ]
        },
        {
            "name": "LLaMAFactory: MMLU@0 Benchmark with Eggachecat",
            "type": "debugpy",
            "request": "launch",
            "program": "${workspaceFolder}/LLaMA-Factory/src/llamafactory/eggachecat_main_with_eggachecat.py",
            "console": "integratedTerminal",
            "cwd": "${workspaceFolder}/LLaMA-Factory",
            "python": "${config:the_anaconda_folder}/envs/eggachecat_llm_eval/bin/python",
            "env": {
                "USE_TORCH": "TRUE",
                "PYTHONPATH": "${workspaceFolder}:${workspaceFolder}/pyvene:${workspaceFolder}/LLaMA-Factory:${workspaceFolder}/LLaMA-Factory/src:${env:PYTHONPATH}"
            },
            "justMyCode": false,
            "args": [
                "eval",
                "--model_name_or_path",
                "${workspaceFolder}/downloaded_models/Meta-Llama-3-8B-Instruct",
                "--template",
                "llama3",
                "--task_dir",
                "${workspaceFolder}/LLaMA-Factory/evaluation",
                "--task",
                "mmlu_test",
                "--lang",
                "en",
                "--n_shot",
                "0",
                "--batch_size",
                "1",
                "--save_dir",
                "${workspaceFolder}/saves/evaluation/mmlu_test_zero_shot_eggachecat"
            ]
        },
        //////////////////////////////////////////////////
        // clean code ////////////////////////////////////
        //////////////////////////////////////////////////
        {
            "name": "Chair:baseline_and_observe_layers:factor_eval:Meta-Llama-3-8B-Instruct",
            "type": "debugpy",
            "request": "launch",
            "program": "${workspaceFolder}/chair/baseline_and_observe_layers_factor_eval.py",
            "console": "integratedTerminal",
            "cwd": "${workspaceFolder}/chair",
            "python": "${config:the_anaconda_folder}/envs/eggachecat_llm_eval/bin/python",
            "env": {
                "PYTHONPATH": "${workspaceFolder}:${workspaceFolder}/pyvene:${workspaceFolder}/LLaMA-Factory:${workspaceFolder}/LLaMA-Factory/src:${env:PYTHONPATH}"
            },
            "justMyCode": false,
            "args": [
                "--model_name_or_path",
                "${workspaceFolder}/downloaded_models/Meta-Llama-3-8B-Instruct",
                "--data-path",
                "${workspaceFolder}/saves/wiki_factor/wiki_factor.csv",
                "--output-path",
                "${workspaceFolder}/saves/evaluation/chair/factor_eval/wiki_factor/Meta-Llama-3-8B-Instruct/baseline_and_observe_layers_factor_eval.json",
                "--task", "eval",
                "--template", "llama3"
            ]
        },
        {
            "name": "Chair:baseline_and_observe_layers:factor_eval:Llama-2-7b-hf",
            "type": "debugpy",
            "request": "launch",
            "program": "${workspaceFolder}/chair/baseline_and_observe_layers_factor_eval.py",
            "console": "integratedTerminal",
            "cwd": "${workspaceFolder}/chair",
            "python": "${config:the_anaconda_folder}/envs/eggachecat_llm_eval/bin/python",
            "env": {
                "PYTHONPATH": "${workspaceFolder}:${workspaceFolder}/pyvene:${workspaceFolder}/LLaMA-Factory:${workspaceFolder}/LLaMA-Factory/src:${env:PYTHONPATH}"
            },
            "justMyCode": false,
            "args": [
                "--model_name_or_path",
                "${workspaceFolder}/downloaded_models/Llama-2-7b-hf",
                "--data-path",
                "${workspaceFolder}/saves/wiki_factor/wiki_factor.csv",
                "--output-path",
                "${workspaceFolder}/saves/evaluation/chair/factor_eval/wiki_factor/Llama-2-7b-hf/baseline_and_observe_layers_factor_eval.json",
                "--task", "eval",
                "--template", "llama2"
            ]
        },
        {
            "name": "Chair:baseline_and_observe_layers:tfqa_mc_eval:Meta-Llama-3-8B-Instruct",
            "type": "debugpy",
            "request": "launch",
            "program": "${workspaceFolder}/chair/baseline_and_observer_layers_tfqa_mc_eval.py",
            "console": "integratedTerminal",
            "cwd": "${workspaceFolder}/chair",
            "python": "${config:the_anaconda_folder}/envs/eggachecat_llm_eval/bin/python",
            "env": {
                "PYTHONPATH": "${workspaceFolder}:${workspaceFolder}/pyvene:${workspaceFolder}/LLaMA-Factory:${workspaceFolder}/LLaMA-Factory/src:${env:PYTHONPATH}"
            },
            "justMyCode": false,
            "args": [
                "--model_name_or_path",
                "${workspaceFolder}/downloaded_models/Meta-Llama-3-8B-Instruct",
                "--data-path",
                "${workspaceFolder}/TruthfulQA",
                "--output-path",
                "${workspaceFolder}/saves/evaluation/chair/truthfulqa/Meta-Llama-3-8B-Instruct/baseline_and_observe_layers_tfqa_mc_eval.json",
                "--task", "eval",
                "--template", "llama3"
            ]
        },
        {
            "name": "Chair:baseline_and_observe_layers:tfqa_mc_eval:Llama-2-7b-hf",
            "type": "debugpy",
            "request": "launch",
            "program": "${workspaceFolder}/chair/baseline_and_observer_layers_tfqa_mc_eval.py",
            "console": "integratedTerminal",
            "cwd": "${workspaceFolder}/chair",
            "python": "${config:the_anaconda_folder}/envs/eggachecat_llm_eval/bin/python",
            "env": {
                "PYTHONPATH": "${workspaceFolder}:${workspaceFolder}/pyvene:${workspaceFolder}/LLaMA-Factory:${workspaceFolder}/LLaMA-Factory/src:${env:PYTHONPATH}"
            },
            "justMyCode": false,
            "args": [
                "--model_name_or_path",
                "${workspaceFolder}/downloaded_models/Llama-2-7b-hf",
                "--data-path",
                "${workspaceFolder}/TruthfulQA",
                "--output-path",
                "${workspaceFolder}/saves/evaluation/chair/truthfulqa/Llama-2-7b-hf/baseline_and_observe_layers_tfqa_mc_eval.json",
                "--task", "eval",
                "--template", "llama2"
            ]
        },
        {
            "name": "Chair: analyze",
            "type": "debugpy",
            "request": "launch",
            "program": "${workspaceFolder}/chair/analyze_chair.py",
            "console": "integratedTerminal",
            "cwd": "${workspaceFolder}/chair",
            "python": "${config:the_anaconda_folder}/envs/eggachecat_llm_eval/bin/python",
            "env": {
                "PYTHONPATH": "${workspaceFolder}:${workspaceFolder}/pyvene:${workspaceFolder}/LLaMA-Factory:${workspaceFolder}/LLaMA-Factory/src:${env:PYTHONPATH}"
            },
            "justMyCode": false
        },      
        {
            "name": "Chair: analyze::per token",
            "type": "debugpy",
            "request": "launch",
            "program": "${workspaceFolder}/chair/analyze_chair.py",
            "console": "integratedTerminal",
            "cwd": "${workspaceFolder}/chair",
            "python": "${config:the_anaconda_folder}/envs/eggachecat_llm_eval/bin/python",
            "env": {
                "PYTHONPATH": "${workspaceFolder}:${workspaceFolder}/pyvene:${workspaceFolder}/LLaMA-Factory:${workspaceFolder}/LLaMA-Factory/src:${env:PYTHONPATH}"
            },
            "justMyCode": false
        },   
        {
            "name": "Chair: analyze::time series",
            "type": "debugpy",
            "request": "launch",
            "program": "${workspaceFolder}/chair/analyze_chair_time_series.py",
            "console": "integratedTerminal",
            "cwd": "${workspaceFolder}/chair",
            "python": "${config:the_anaconda_folder}/envs/eggachecat_llm_eval/bin/python",
            "env": {
                "PYTHONPATH": "${workspaceFolder}:${workspaceFolder}/pyvene:${workspaceFolder}/LLaMA-Factory:${workspaceFolder}/LLaMA-Factory/src:${env:PYTHONPATH}"
            },
            "justMyCode": false
        },
        // {
        //     "name": "Chair:baseline_and_observe_layers:mmlu_test:Meta-Llama-3-8B-Instruct",
        //     "type": "debugpy",
        //     "request": "launch",
        //     "program": "${workspaceFolder}/LLaMA-Factory/src/llamafactory/eggachecat_main_baseline_and_observer_layers.py",
        //     "console": "integratedTerminal",
        //     "cwd": "${workspaceFolder}/LLaMA-Factory",
        //     "python": "${config:the_anaconda_folder}/envs/eggachecat_llm_eval/bin/python",
        //     "env": {
        //         "USE_TORCH": "TRUE",
        //         "PYTHONPATH": "${workspaceFolder}:${workspaceFolder}/pyvene:${workspaceFolder}/LLaMA-Factory:${workspaceFolder}/LLaMA-Factory/src:${env:PYTHONPATH}"
        //     },
        //     "justMyCode": false,
        //     "args": [
        //         "eval",
        //         "--model_name_or_path",
        //         "${workspaceFolder}/downloaded_models/Meta-Llama-3-8B-Instruct",
        //         "--template", 
        //         "llama3",
        //         "--task_dir",
        //         "${workspaceFolder}/LLaMA-Factory/evaluation",
        //         "--save_dir",
        //         "${workspaceFolder}/saves/evaluation/chair/mmlu_test/Meta-Llama-3-8B-Instruct/mmlu_test_baseline_shot_3",
        //         "--task",
        //         "mmlu_test",
        //         "--lang",
        //         "en",
        //         "--n_shot",
        //         "3",
        //         "--batch_size",
        //         "1"
        //     ]
        // },
        {
            "name": "Chair:baseline_and_observe_layers:mmlu_test:Meta-Llama-3-8B-Instruct",
            "type": "debugpy",
            "request": "launch",
            "program": "${workspaceFolder}/chair/baseline_and_observe_layers_llama_factory.py",
            "console": "integratedTerminal",
            "cwd": "${workspaceFolder}/LLaMA-Factory",
            "python": "${config:the_anaconda_folder}/envs/eggachecat_llm_eval/bin/python",
            "env": {
                "USE_TORCH": "TRUE",
                "PYTHONPATH": "${workspaceFolder}:${workspaceFolder}/pyvene:${workspaceFolder}/LLaMA-Factory:${workspaceFolder}/LLaMA-Factory/src:${env:PYTHONPATH}"
            },
            "justMyCode": false,
            "args": [
                "eval",
                "--model_name_or_path",
                "${workspaceFolder}/downloaded_models/Meta-Llama-3-8B-Instruct",
                "--template", 
                "llama3",
                "--task_dir",
                "${workspaceFolder}/LLaMA-Factory/evaluation",
                "--save_dir",
                "${workspaceFolder}/saves/evaluation/chair/mmlu_test/Meta-Llama-3-8B-Instruct/mmlu_test_baseline_shot_3",
                "--task",
                "mmlu_test",
                "--lang",
                "en",
                "--n_shot",
                "3",
                "--batch_size",
                "1"
            ]
        },
    ]
}